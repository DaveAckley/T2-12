{138}  -*- text -*-
[0:

Thu Oct  1 04:25:46 2020 OK so we're closing in on two years of T2sday
Updates and we need a foggen plan.

Random thoughts first.

 - Perhaps target T2sup#3100 as a series finale?  At some level.  That
   would be somewhere around next April on the current schedule?

 - We need a downshift and tool-strengthening push on intertile
   events.  Don't think about debugging; think about visualization.

 - Simple neural network structures, with many atoms cooperating in a
   single neuron+dendritic tree+axon, is a tremendously natural idea
   for running on the grid.  Imagine 'sensory data' on one grid edge,
   'motor commands' on another, and dynamic neural structures
   remapping data, abstracting and instantiating, between them.  Or
   imagine 'sensorimotor' at the top of the grid, and 'abstract mind'
   at the bottom of the grid, and the flow goes down and then up
   again.  So low-order sensorimotor loops can be resolved more
   quickly than higher-order computations.

 - When/if do we start publishing this stuff in science/engineering
   venues?

:0][1:

Thu Oct  1 04:52:23 2020 And what might a 'downshift and
tool-strengthening' look like?

 - A mile-high view of event processing; something that suggests some
   kind of first principles for resolutions of race conditions.

 - The 'hard part' is the distibuted commit?  As long as the event
   originator can decide to flush an event things are easy.

 - We want more of a falling-toward-consistent solution.

 - Where is the stigmergy in event processing?  Could we somehow mark
   the floor as 'dir8 claims this site+radius 2'?

 - The CDM stigmergy move was shifting stuff into the file.  Could we
   ship an entire update to everybody without even holding a lock?
   And ask them if they can apply it cleanly?  And when they all say
   yes we say: Shift assumptions to it's happening.

 - How about that?  Part of the protocol can be a shift in
   responsibility in the case of failure or timeout.  At first the
   passive sides believe their ground state is to break the lock and
   forget the event.  But if they see that event processing has
   reached a certain state, they change their belief so that their
   ground state is to apply the change.  As long as everything works
   out without failures, the ground state beliefs shouldn't matter,
   but -- the thought is -- once timeouts start occuring, the ground
   states provide the justification for what the failure-recovery code
   in all the different components should do.

[2: Thu Oct  1 05:07:35 2020

 - Another thought is to couple to a larger failure mode, at the ITC
   level.  If an event fails, trigger a renegotiation of all involved
   ITCs.  All existing events touching the ITC blow up uncommitted,
   all involved locks are released.

 - So each side needs to be able to get to that state successfully
   (i.e., without FAIL), even though they'll hear about it in somewhat
   different ways.

 - But again, we need big-picture, out-of-the-weeds views on this.

Another point of view is that we should do retries for apparently lost
event process messages, and be willing to wait a very long time, until
we get explicit ACK or NAK on any given state.

That feels like going in the wrong direction, of course.

But what does the right direction feel like, here, really?

:2]
:1]
[3:

Thu Oct  1 05:19:39 2020 Restate fundamentals.

We have the race to get a lock.  That's tough because A might be
asking B for a lock on region rA, while B is asking C for a lock on a
region rB that doesn't overlap rA, while C is asking A for a lock on a
region rC that overlaps rB but not rA.  All those requests can be
satisfied pairwise (if I set the story up right), but not all locks
can actually be granted.

Actually I don't think I set the story up right.

:3]
[4:

Thu Oct  1 05:38:49 2020 Think about log centralization tools.

 - DO THE TRACE-TO-DISK ON FAIL.  DO IT NOW NOW NOW
[6:

Fri Oct  2 00:51:14 2020 Reviewing the Trace system.[7:

Fri Oct  2 02:34:06 2020 Well, have been re-reading through
/home/t2/MFM/src/drivers/mfmt2/notes/202006120116-notes.txt here,
re-exposing the brain to intertile, the yoink protocol, Weaverville,
and the rest.

And thinking: What about another stigmergic 'simplification'?  What
if we ditched the 32 active EventWindows, and ditched the 32 passive
EventWindows?  What if we gave an active and a passive EventWindow To
Every Visible Site In The Tile?  [8:

Fri Oct  2 04:04:40 2020 The idea is that the current EW#s are like
the CDM SKU's and tag#s -- just more state that can get out of sync
and go bad.[9:

Fri Oct  2 12:26:51 2020 Good second sleep.

[10:

Fri Oct  2 12:27:01 2020 So, with the CDM10 story, we have clear
separation of timescales between creating a new .mfz (relatively slow)
and disseminating it to the grid (relatively fast).  That seems, at
best, far less true with event processing.

One thing it would be good to know is whether any of our issues
involve MFM packet loss.  It's pretty easy to believe our CURRENT
issues anyway are just bugs even though all packets arrive
(eventually).

I guess in absence of a clear design alternative (still), our current
mission is still to do the snap-to-disk trace logging.  And Hey Why
Not Make A Flash Command For That?
[11:

Fri Oct  2 12:35:35 2020 So let's design a snap-trace-to-disk
mechanism; that's nice and conventional and non-threatening.

 - Manage some assigned pool of disk space; flushing oldest
   (unmarked?) traces automatically to gain space.

 - Have a concept of a 'trace group', identified by a random label,
   to link trace files across multiple tiles.

 - Have a separate directory for each trace group?  Or just a separate
   trace file per trace group, which get merged into a trace group
   directory as a second step.  Maybe keep the snap target flat: Just
   a lex sequence number plus a 32 bit random tag in hex.

 - On a fail-to-top-level, or via an explicit call, send a Persist
   History flash packet with a one hop TTL.  If it's by an explicit
   call, I guess we then go about our business and wait for the flash
   timer to trigger locally as usual, but if it's a fail-to-top I
   guess we block, doing nothing else, until the appropriate time?
   But what is 'nothing else'?  Unclear.  Maybe we snap-to-disk out of
   turn in that case, then go ahead and crash.

 - Disk structure:
     SNAPDIR/ctrl/seqno.dat    ; Last sequence number used, u32 netorder
     SNAPDIR/snap/<FILE>       ; Trace files FILE lex(seqno)-hex(label).dat
     SNAPDIR/save/<DIR>        ; Unified trace files for DIR hex(label)

[13: Fri Oct  2 12:58:34 2020

 - Going to need a way to move files between nearby tiles!  Been
   seeing that coming, with mixed emotions, but here it is!  The idea
   is we should be able to plug a serial cable into the middle of the
   grid, 'centralize' the nearby files associated with a given label,
   and run the curses Weaver right there to examine things.

:13]
[12:

Fri Oct  2 12:53:27 2020 Hmm maybe include relative address in the
filename, so we can tell who's where and who triggered this group.

Also maybe we want to separate the explicitly-saved unified trace
files from the automatically-managed ones, so that the automatic disk
space management only applies to the the latter.

Well, but we can have that simply by saying it's only the SNAPDIR/snap
directory that's auto-space-managed..

:12]

:11]

:10]

:9]

:8]

:7]

:6]
:4]
[5:

Thu Oct  1 13:03:48 2020 Getting ready to move the flag.  Feel like I
haven't gotten to any decent 'big picture' on event processing..

Perhaps if we get that story set up right we can derive something like
the latest points at which an event can get blown up..

But really I want to move towards timeouts and retransmits.  Just like
CDM now rides out packet losses, I want to get event processing to
look more like that.  So that any party can say 'I think the situation
is this' and that should trigger some explicit response from everybody
that hears it.  I guess the response might be just doing the next
step, if that's what comes next from some participant's point of view,
or a NAK if that's not the situation on the recipient's view, or an
ACK if that is the situation but the recipient is not next to move.

And we need to remember that Things Will Still Blow Up.  So we need,
for example, mechanisms to count and report failures.

:5]
[14:

Fri Oct  2 13:19:07 2020 OK, so have a call at the top of the hour.
Let's think about moving single files between tiles.

OPTIONS!

 - Extend CDM somehow.  CDM owns /dev/itc/bulk, so if our file was
   going to move on that device..

 - Extend MFM somehow.  Have the file move via /dev/itc/mfm/*.  Or by
   /dev/itc/flash?  Those both seem uncommonly bad ideas.  For moving
   background bulk data.

 - Create yet another minor device in the itc_pkt LKM.  Like
   /dev/itc/ptp or /dev/itc/misc or something.  But I mean we have to
   stop meeting like this.

[15:

Fri Oct  2 13:23:48 2020 Of those three I guess extending CDM feels
least worst.  Creating another device seems 'clean' but then all the
userspace issues just recapitulate.

 - It's perl, easy to code, and already slow to run and slow to
   transmit.

 - Could have like outbound/ and inbound/ directories.  Maybe per-ITC?
   Maybe.  Dump a file in /cdm/ptp/outbound/NE/, and it will get
   shipped one hop toward the NE.  And once it arrives there intact
   (in /cdm/ptp/inbound/SW/, presumably), it will get deleted from
   here.

 - Now, as long as we're (thinking of) doing this, we could extend it
   to the multihop worm-routed syntax as well.  Like, drop a file in
   /cdm/ptp/outbound/223/, and it'll get shipped east (dir8==2) two
   tiles and then southeast (dir8==3) one tile.  And it'll land there
   in /cdm/ptp/inbound/766/, showing where it came from.

 - Now what do we do about file names?  Aren't we heading right back
   towards SKUs here?  Could the files be anonymous?  Like you can
   only send one file -- 'the file' -- and then it's up to sender and
   receiver to determine what it means?  Or I suppose we could fake up
   some metadata, like an on-the-fly cdm10 header, to supply a label
   and a size and what-not?

 - We could include a few bytes of digest in every packet, rather than
   percent mapping the whole thing?  But I guess if there's a
   non-trivial chance of a false positive that would be bad; we
   wouldn't know how far back to chew the file.

 - Well how about a real cdm10 header?  Or perhaps an unsigned one?  A
   sixteen byte label is pretty tight if we want hex(seqno+label) in
   it

:15]


:14]
[16:

Fri Oct  2 13:43:10 2020 I wonder about creating a literal socket..
Like, you configure it with "223" or whatever, and I guess you
configure the far end with "776" too?, and then you treat it like an
regular (network?) socket.  What about if it's a virtual TTY, and we
could use like sz/rz over it?

I REALLY DON'T WANT TO OPEN INTERTILE UP TO GENERAL UNIX TRAFFIC!

[17:

Fri Oct  2 13:50:56 2020 What if it's like a datagram though.  No
stream reliability, no guarantee of packet delivery, just some
checksum for plausible packet integrity if it does arrive.  Still
scary.

:17]
:16]
[18:

Sat Oct  3 02:14:56 2020 Well.

We do have all that 'non-standard' space just sitting there in the
packet header.

Suppose.

Suppose we implemented IXM-style source routing.

Like:

  !"#$%&', 0x20..0x27: End-of-address code: Deliver here.

 01234567, 0x30..0x37: Source-routed packet arriving from byte[0]&0x7

                       Delete byte[0] (which shall match the arrival
                       dir) and examine byte[1].  Route rest in direction
                       dir8==BYTE-'0'.  Insert ((dir8+4)%8)+'0' after
                       ' ' if that is encountered before any other
                       non-'0'..'7' byte.

[19:

Sat Oct  3 04:50:25 2020 So, a couple points:

(1) The PRU code basically doesn't care what the first byte of the
    packet is; it doesn't examine it.  But
(2) The PRU code writes the source direction of the packet into the
    bottom three bits of the first byte of the packet regardless of
    what is there.

So any 'non-standard' packet types better be assigned in aligned
groups of eight, and be prepared to have their bottom three bits
smashed in transit.


:19]

:18]
[20:

Sun Oct  4 01:12:17 2020 OK, well where the hack are we, here?  Are we
really thinking of trying to implement source-routed packets?[21:

Sun Oct  4 01:17:58 2020 How about we just fake that inside CDM
packets?  That's minimally invasive.  Then it's all about how to hook
the packets into local userspace.

What about the outbound and inbound file idea instead?  So you can
only send one file at a time to any given destination, so the filename
issue goes away and the 'SKU' is implied by the source route.

[22:

Sun Oct  4 01:27:27 2020 OK come on enough with the 'big picture'
-- sheesh -- we have to hack.  So,

TODO

 - Create a variant CDMap called 'UDM10' -- meaning something like
   unsigned/unique data map/manager.  Use the bytes of the mSignature
   field for a

 - Have cdmctl acquire a new command (or start a new cdm program):
   'cdmctl send ROUTE FILE', and it creates a UDM from the file and
   dumps it in /cdm/ptp/outbound/ROUTE

 - Make a new UDManager.pm in cdm/ that watches /cdm/ptp/outbound,
   announces files via source routed packets, the whole megillah.

Jeez it's all a lot of hair.  Can't we make it all manual like zmodem?

How do unix sockets work again?  Named pipes?
[23:

Sun Oct  4 03:19:52 2020 Well, we have a perl spike --
./pairunixdomain.pl s -- setting up a non-blocking unix domain socket
and reading data from connections, and closing the connections down
when they finish.  And we can send big chunks of data and have it all
get there with:

   # cat 202010010425-big-picture-notes.txt | nc -NU /home/t2/PUD.sock

(NOTE: THAT'S AFTER WE INSTALLED netcat-openbsd --
    # apt-get install netcat-openbsd
)
[24:

Sun Oct  4 03:24:07 2020 So if we had a solution for specifying the
desired routing, plus a hacked-up CDM level source routing mechanism,
we'd be close to having all the pieces.[25:

Sun Oct  4 03:36:44 2020 OK well it is after all a two-way connection
we've got, so we can report status back to the client that way.  Like
at the moment, we're showing:

  ..
    BONG 1601804176
    CONNECTION IO::Socket::UNIX=GLOB(0xa61f90)
    DATA FROM IO::Socket::UNIX=GLOB(0xa61f90): ZNLDONEW
    FORCED GDBY TO IO::Socket::UNIX=GLOB(0xa61f90)
    BONG 1601804181
  ..

on the server, and

    root@beaglebone:/home/t2/T2-12/notes# echo -e "ZNLDONEW\n foNGe\ndslsd" | nc -NU /home/t2/PUD.sock
    ZABODO BAYEY:IO::Socket::UNIX=GLOB(0xa61f90)
    root@beaglebone:/home/t2/T2-12/notes#

on the client.

So we could make the first line be a command, like "SEND addr" or
"RECV addr", and those would need to match up like in the good old
zmodem days.  "SEND 1" on a tile would need a "RECV 5" on the tile to
its northeast.  Our server would route some header/request to 1, and
the NE server would ack or nak after scanning its active connections.

Server side could retry every ten seconds or so perhaps, for as long
as its client was willing to wait.

When a match is established, server side starts shipping data packets,
and client side starts acking them, with maybe enough digest data to
believe each packet.  As client sees valid checksums, it delivers the
bytes to its receiver and acks the length delivered.

Eventually client could nak when a problem is detected, and have the
server resend that packet.  So server needs to buffer all the data its
sent that hasn't been acked (in typical reliable stream style doh) so
it can retransmit as needed.

Eventually the whole file is transmitted, and the server announces
that with a final packet including the whole-file checksum.  At that
point the server closes its source connection, and the client closes
its sink connection, and that's the end of that delivery.

It could be "SEND toroute FILENAME" and "RECV fromroute TODIR".

Now given that we're going bidirectional here anyway, so command line
pipes may not be the most useful, we could support multifile
connections if we wanted to.  Sender would need to specify the data
length of each file, and then (precisely) that much data, and then a
send command for the next file.

It would be nice to know the data length for progress reporting in any
case.

And the only reason we're interposing CDM in the middle of all this is
because it's hogging /dev/itc/bulk so nobody else can get to it.

:25]

:24]
:23]
:22]

:21]

:20]
[26:

Sun Oct  4 09:00:34 2020 So let's keep it as simple as we can manage,
for starters.

TODO

[27: Sun Oct  4 10:39:11 2020 First cut, as PacketCDM_S.pm,
DONE :27] - SRPacket.pm for source-routed packets at the CDM level

 - SRManager.pm to generate and handle SRPackets

 - A Unix-domain datagram server?  Or just stream?

:26]
[28:

Sun Oct  4 10:51:51 2020 OK we need a first specific SR packet type to
debug with.  Announce a file?  SF?  Sure.[29:

Sun Oct  4 12:47:09 2020 OK, so we sent an SR F packet twice around a
loopback cable and successfully landed it on unimplemented code in
ourselves.  Next stop would be to spike-out an SRManager to (generate
and) handle the announcement, but I think I need to switch to
finishing off the October ComputingUp while it's still the
weekend.[30:

Mon Oct  5 00:14:59 2020 Well, it won't quite be the weekend.  Will
push out the episode today.

:30]

Pulling t2 dir, but not committing yet..

:29]

:28]
[31:

Mon Oct  5 00:15:25 2020 Starting on SRManager.pm[32:

Mon Oct  5 01:47:36 2020 OK, have SRManager listening and accepting
connections on /cdm/sockets/xfer.sock, and we need another class to
represent an xfer state or something.  So far, now, when we accept a
connection, we don't even know whether they want to send a file,
receive one, or who knows what combination or else.

I wonder how the zmodem protocol actually works.[33:

Mon Oct  5 01:56:19 2020 Well it's non-trivial, specific, and of course
truly ancient, based on
https://stackoverflow.com/questions/9611000/understanding-the-zmodem-protocol

If we could somehow act like a transparent point-to-point connection,
we could possibly tunnel an existing zmodem implementation over it..

Or ssh for that matter.  Or netcat.  It's just getting the route
specified that's the problem.
[34:

Mon Oct  5 02:46:08 2020 So, once again, let's just pound it out.
We'll make an SREndpoint, say, that represents a local connection to
the socket, regardless of its purpose.  We'll have packet formats for
talking to ..[35:

Mon Oct  5 03:20:02 2020 Actually, screw it.  Let's just have a single
one-line exchange, first from client to SREndpoint, and then back,
followed by transparent byte delivery.  Client connects and sends the
route they want to connect on.  SREndpoint checks if there's a pending
request from that route, .. Aand we have a race on establishing the
connections.  We could make the local users specify whether they wish
to be 'clients' or 'servers' when they connect; then we'd only send
connection requests from the client to the server.

And it's nice: If a user connects and says '1238', that means they
want to be a client, connecting to route 1238 (go NE, then ET, then
SE, then you're there).  But if a user connects and says '8123', that
means they want to be a server, listening for connections from that
tile.

We capture the route, check if the named SREndpoint exists, and then
go from there.  For clients, we issue an SR_C packet aimed at 1238 and
wait for a reply, retrying a few times ad hoc.  For servers, we sit
and wait for inbound SR_C packets from 8123.

Eventually either a (final) timeout happens or a connection is made,
and we issue our one-line response to the local users: 'CONNECTED' or
'TIMEOUT', perhaps other errors as needed.

Then we just start shoveling bytes in both directions, using SR_D
packets that send a variable size packet, with a stream index, and a
checksum, plus a stream index for the reverse direction
acknowledgment.

I guess we just try to read a maximum length chunk from our local
user, but we make an SR_D packet out of whatever non-zero length we
get, and stick it on an outbound packet queue.  We could do the stream
index in terms of SR_D packet count rather than the underlying stream
byte count, and so an arriving SR_Ds would tell us how many SR_Ds we
could drop from the front of our outbound queue.

I guess we need to support a zero-length SR_D so we can ack packets
even if we have nothing to ship, or have an SR_A packet that just acks
without data to send instead.

If we detect a checksum error or a break in the stream index, we send
an SR_R retransmit request, which resets the receiver's stream back to
the requested index, and go from there.

If we see EOF from our local user, we put an SR_Q quit packet into our
outbound queue.  I guess we'll need to deal with half-closed
connections.

I guess really we need an inbound queue, too, to have a place to stick
arrived SR_Ds before our local user has read them.  Which also pushes
back toward byte indices instead of packet indices, since the user is
not strictly constrained to take a whole packet's worth, whatever that
turns out to be in each case, at once.
[36:

Mon Oct  5 03:49:53 2020 And hey look, we ought to be able to do like:

    $ echo "1238" | cat - MYFILE | nc -NU /cdm/sockets/xfer.sock

to send MYFILE to 1238.  And I guess like

    $ echo "8765" | nc -NU /cdm/sockets/xfer.sock | tail +2 > YOURFILE

to receive it three tiles away (ignoring errors and timeouts)?

:36]

:35]

:34]
:33]

:32]

:31]
[37:

Mon Oct  5 10:03:49 2020 After second sleep.

Let's use '.' instead of '8'?  ..Except we want the range to be
contiguous so that 'return $packet =~ /^[\x80-\x87][\xb0-\xb8]/;'
works.

Except we're now calling 'encodeRoute' and 'decodeRoute' all over the
place anyway; we could map between '.' <-> '\xb8' there, right?

Leave it for now, I guess; leave it '8'.

:37]
[38:

Mon Oct  5 10:11:03 2020

TODO:

 - Add netcat-openbsd to the list of packages we want

:38]
[39:

Mon Oct  5 10:27:19 2020 Roughing out SREndpoint.pm.

:39]
[40:

Mon Oct  5 13:11:40 2020 OK, making progress but also getting a fair
bit of untested complexity accumulating and making me anxious.

So, we have to be explicit about whether an SREndpoint is 'client' or
'server' -- we can't tell the difference by the route, because we need
the route to be the same for both, to match up the two ends.  So I
guess a three-way mUserMode?  In (undef, 'client', 'server')?  Or
(0,1,-1)?

Or mServerMode, in (-1 => unknown, 0 => client, 1 => server)?

But wait is this really true?  What's wrong with having two
connections on a given route, one being server and one being client?
As long as they match up.

So if we get a SR C over the net, that means there is a client at the
far end for sure, and we should look for an *existing* server on that
route, NAK the C if we don't have one, and *don't* create one.  So we
only create SRes when local users connect and then tell us what kind
of endpoint they are.  And it's up to the user to start up the server
first, so that it's already there when the SR C arrives.
[41:

Mon Oct  5 13:26:14 2020 So we can go with 8xx routes meaning 'server
to xx' and 'xx8' routes meaning 'client of xx'.  And don't need
mUserMode.  Right?

:41]
:40]
[42:

Tue Oct  6 00:14:35 2020 OK, so now we're an SREndpoint configured as
a client.  How do we know if we've received an SR_R CONNECTED?  I
guess, mTheirSeqno is >= 0?

:42]
[43:

Tue Oct  6 04:51:19 2020 So, using the route itself as the address is
kind of weird.  It means that for two tiles to connect up they must
not only target each other but they must choose the exactly
complementary paths.  Using relative coordinates for addresses would
make more sense, but that leaves us without a source route.

We could still work in terms of source routes, but then take an
inbound return address and collapse into a relative coordinate, and
use that as the key to the endpoint map.  Then even if the two ends
specified non-complementary routes, as long as the routes collapsed to
the same coordinates, the connection would be made.

Is that maybe a plausible compromise?  It still suffers/offers the
loopback cable aliasing problem/debugging trick.[44:

Tue Oct  6 05:26:25 2020 And it recreates the client/server endpoint
ambiguity that I thought I had to deal with, before.

:44]

It does expand our assumptions about regional grid flatness.  But
probably not significantly more than, say, flash traffic already does.

Second sleep.

:43]
[45:

Tue Oct  6 10:43:49 2020 S/C+Relative coord keys. GO GO GO  [46:

Tue Oct  6 10:51:20 2020 Actually wait, let's get Computing Up posted
first; it's way overdue again.[47:

Tue Oct  6 14:11:58 2020 OK, 40th conversation posted, and the pot
roast sitting in the oven at 250F.

:47]

:46]

:45]
[48:

Wed Oct  7 01:43:00 2020 OK, last couple hours here debugging through
the SREndpoint::getKey isclient+offset structure.  Have gotten to
getting a server started and configured, but it doesn't know what to
do with transparent data when it shows up.

Let's now get a matching client started, and then connecting to the
server.. [49:

Wed Oct  7 02:14:26 2020 OK well that was stupid.  I made a server
on route '83', meaning it was listening for clients connecting from
the SE.  Than I made a client on route '38', meaning it was trying to
connect to a server in the SE.  But we're on a SE<>NW loopback so the
client request actually arrives from '87'.  So that client address
didn't match what the server was listening for.

But now serving on 83, and connecting on 78, we have managed to call
the non-existent SREndpoint::acceptConnection method.  Step by step.

:49]

:48]
[50:

Wed Oct  7 06:10:17 2020 Still down in the weeds of DataQueue
interacting with SREndpoints, but out of gas now.

Second sleep.

:50]
[51:

Wed Oct  7 11:05:13 2020 OK, so.  Part of the overnight muddle was we
started out with separate XmitQueue.pm and RecvQueue.pm, but merged
them into DataQueue.pm somewhere along the way, and haven't done a
very good job of it so far.

Problems include

 - Method named assuming the xmit side -- that adding to the queue
   happens from a filehandle but, but removing from the queue always
   succeeds up to a window-bounded size.

Well, that anyway.

How should we name/structure it?

 - acceptFromFH.  Read up to what the buffer can take from the FH.
   Detect and store EOF on the FH so that it can be delivered later
   when the buffer has gone empty.  [used on sre->mXmitQueue]

 - deliverToFH.  Write len bytes -- up to whatever the buffer has --
   to the FH, without blocking.  Call retireToSeqno(mFrontSeqno+len)
   to remove len bytes from the front[62: changing mFrontSeqno but not
   mNextSeqno. :62].  [used on sre->mRecvQueue]

 - appendString.  Insert as much of string as will fit in the
   buffer, return amount of string taken[61:  and advance mNextSeqno
   by that amount  :61].  [used on sre->mRecvQueue]

 - nextString.  Return up to what's requested bytes as a string,
   starting from mNextSeqno, without allowing (mNextSeqno -
   mFrontSeqno) to become larger than the window size.  Adjust
   mNextSeqno to track whatever is returned. [used on sre->mXmitQueue]

 - retireToSeqno.  Delete to the given seqno from the front of the
   buffer, incrementing mFrontSeqno by the amount deleted.

 - retryFromSeqno. Reduce mNextSeqno to the given seqno, as long as
   that is >= mFrontSeqno.

 - markEOF.  Set EOF on the buffer, meaning that: (1) acceptFromFH and
   appendString will no longer add anything to the buffer, and (2)
   atEOF() will start returning true as soon as the buffer is empty.

:51]
[52:

Wed Oct  7 15:18:43 2020 Got as far as SREndpoint::handleResponse
needing to deal with mResult OK.[53:

Thu Oct  8 00:10:45 2020 OK come on ship data tonight GO GO GO

:53]

:52]
[54:

Thu Oct  8 05:29:06 2020 Well still not there.  The client is dealing
with SR_R mResult OK but it's got also to send an SR_R back to the
server as well.  We were planning on letting the server go transparent
when it sees the first data packet, but that means the client would
have to speak first and the server couldn't xmit even if it had data.

Which maybe isn't necessarily such a bad thing..

Actually wait.  What if we just have the client send a data packet
immediately upon going transparent, and just let it be zero length if
there isn't actually anything ready to send on the client side?  Then
server does go transparent on first data packet arrival.

Maybe.

Second sleep.

:54]
[55:

Thu Oct  8 12:03:15 2020 Working for last 90 minutes or so; now up to
DataQueue::maybeReceiveData.  What happens there?

Hmm, maybe it should be SREndpoint::maybeReceiveData.  Need to access
both DataQueues, for accepting inbound data and retiring outbound
data.
[56:

Thu Oct  8 13:19:52 2020 OK, about time to move the flag, but the
current issue is that somewhere along the way our server coordinates
are (not) getting negated so that when the first D packet arrives, it
gets delivered (back) to itself (the client) rather than to the server
for which it was intended.

When a packet arrives at an endpoint, the packet's route is all in the
'from', and it's been both sequentially reversed and with each step
dir8-inverted.  So a packet with mRoute 7338 arrive with its mRoute
set to 8773.[57:

Thu Oct  8 13:42:24 2020 Time to pack up.  But since we're adding
coordinates and addition commutes, sequential order shouldn't matter.
But dir8 reversing should change the sign of the collapsed result, no?

Have to check it through more carefully.

Risky bits off.

:57]

:56]

:55]
[58:

Thu Oct  8 23:38:48 2020 OK GO GO GO darn it get bytes moving.

:58]
[59:

Fri Oct  9 02:36:58 2020 OK a couple issues:

 - When do we ack incoming bytes?  I originally thought we'd only ack
   them once we actually delivered them to the local user.  So when we
   generate an ack in a reply, we're currently giving the mFrontSeqno
   of the mRecvData queue.

   Aand why is the mFrontSeqno getting advanced once we send stuff to
   the local user?  I do see output showing up..

:59]
[60:

Mon Oct 12 07:45:12 2020 OK, well, was working on We Are Coders over
the weekend, but I'd like to take one more run at getting this
starting to work before T2sday.

Assuming we have to ack bytes once they reach mRecvData, not once they
ship to the local user.  So the value to ack from the rd is
mFrontSeqno+length(mBuffer)?  What does mNextSeqno do in this context?
It's always ignored?  Implicitly equal to mFrontSeqno?

Because we don't have to allow for the local user to drop bytes,
there's no distinction between mFrontSeqno and mNextSeqno.  Whenever
we write bytes to the local user they are gone for good.

Well which is it?  If mNextSeqno is mFrontSeqno+length(buffer), then
mNextSeqno is what we should ack with -- it's the next sequence number
we're expecting from sender.  And we should increment it whenever we
add bytes to the buffer.  That seems kind of plausible.

If mNextSeqno is mFrontSeqno, then it's what we're next going to ship
to the local user.  Which isn't ridiculous, but it isn't as
compelling.

So we need to focus on the API methods above that are local user
facing, and let's try to ensure mRecvData.mNextSeqno stays at
mFrontSeqno+length(buffer), and we use mNextSeqno in acks.

:60]
[63:

Mon Oct 12 08:12:01 2020 Well, so far from that review all that
happened is that this line in DataQueue::maybeShipData:

    $pkt->{mAckRecvSeqno} = $rq->{mFrontSeqno};

got changed to this line

    $pkt->{mAckRecvSeqno} = $rq->{mNextSeqno};

so.. let's see what that does..[64:

Mon Oct 12 08:17:48 2020 So looks like first packet (containing "n\n")
got into mRecvData and out to local user okay.  [65:

Mon Oct 12 08:22:17 2020 And it advanced its mNextSeqno from 1993 to
1995.. but it didn't acknowledge anything back to sender, because it
had nothing it needed to say.  So apparently sender is resending the
"n\n" over and over, and it's getting dropped at the receiver over and
over:

    67.46:PacketIO#0: #9(0,2,0) osn(41575) tsn(1993) Dropping packet; have 1993 got 1995

and then sent again:

    242.97:SREndpoint#16/#13(1,-2,0) osn(1993) tsn(41575): SEND D to 758 ack 41575 seq 1993 len 2

etc.

So, if we feed some data to the other end, so the 'receiver' will have
something to send, will that cause this 1993->1995 "n\n" to stop
getting sent?[66:

Mon Oct 12 08:27:42 2020 No, because our server->client packets are
getting addressed to 813 so the source routing ends immediately and,
I think, they get dropped because there's no client there to handle
them. [67:

Mon Oct 12 08:30:41 2020 So, what really do we think the mRoute on a
server should be?  Actually -- and maybe we should rename one --
there's TWO (or *THREE*) mRoutes in play: SREndpoint::mRoute and
DataQueue::mRoute.  Maybe SREndpoint::mRoute should stay as it is --
813 in this case -- showing that it's a server; but mXmitData::mRoute
should become 138, because that's the return route the server should
use to xmit things to the client?  Is mXmitData::mRoute what's being
used to address the E packet?

:67]

:66]


:65]

:64]

:63]
[68:

Mon Oct 12 09:20:25 2020 So retireToSeqno is updating mFrontSeqno but
not mNextSeqno, is that right?  I thought the idea was that mNextSeqno
would always already be higher than mFrontSeqno (and any retirement
point) so that retireToSeqno would never touch mNextSeqno.

Where DO we think mNextSeqno should get moved?[69:

Mon Oct 12 09:24:24 2020 Well at the moment,

 - appendString increments mNextSeqno.  SRE::maybeReceiveData calls
   that while accepting data.

 - nextString increments mNextSeqno.  But apparently nobody calls
   nextString?[70:

Mon Oct 12 09:29:03 2020 Because, it appears, DataQueue::maybeShipData
repeats a lot of the code of DataQueue::nextString, but not the
mNextSeqno incrementing.  Niiiice.

    sub nextString {
        my __PACKAGE__ $self = shift || die;
        my $max = shift; defined $max or die;
        my $alreadyout = $self->{mNextSeqno} - $self->{mFrontSeqno};
        my $available = length($self->{mBuffer}) - $alreadyout;
        my $windowsize = MAX_DATA_BYTES_IN_FLIGHT;
        my $advance = min($max,$available);
        $self->{mNextSeqno} += $advance;
        return substr($self->{mBuffer},$alreadyout,$advance);
    }

vs
   ..
        my $unacked = $self->{mNextSeqno} - $self->{mFrontSeqno};
        return unless $unacked < MAX_DATA_BYTES_IN_FLIGHT;
        my $available = length($self->{mBuffer}) - $unacked;
        return unless ($available > 0) || ($emptyokopt && $available == 0);
        my $shipamt = min($available, MAX_D_TYPE_DATA_LENGTH);
        my $data = substr($self->{mBuffer},$unacked,$shipamt);
        my $pkt = PacketSR_DE->new();
        $pkt->{mAckRecvSeqno} = $rq->{mFrontSeqno};
        $pkt->{mThisDataSeqno} = $self->{mNextSeqno};
        $pkt->{mData} = $data;
   ..

I guess we want

        $self->{mNextSeqno} += $shipamt;

somewhere after $pkt->{mThisDataSeqno} gets set up?

And I guess perhaps we also want to try to stop sucking quite so much
at coding as if we've been doing it for more than 21 days in our life?

:70]



:69]

:68]
[71:

Mon Oct 12 15:39:46 2020 OK so now we're reaching
SREndpoint::handleQuit and need to do something besides say IMPLEMENT
ME.  Perhaps

 - Ignore it if isClosedNetwork

 - Otherwise set mEOFSeen on mRecvData?

 - And on mXmitData?

 - check if do disconnectNetwork

 - Check if isFullyClosed.  If so, exit?  That's assuming a 'server'
   is only going to serve one client, which, I think for now, is what
   we are thinking..

:71]
[72:

Tue Oct 13 00:28:18 2020 Well, I'm going to commit all this as a WIP.
We got it to transfer a non-trivial file successfully, although
server-side connections don't know how to clean themselves up properly
on closing, so in fact we can't do more than one transfer without
crashing cdm.pl.  Nice.

I wonder if we can do two transfers at once, though?[73:

Tue Oct 13 00:37:02 2020 Well, kind of.  But the current hack actually
EXITS when it's gotten to the end of a file, so the second file (not
to mention all the rest of cdm.pl, hmm) doesn't have a chance.

But none of this does anything until somebody connects, so..


:73]

:72]
[74:

Fri Oct 16 23:04:13 2020 Well another slow back after T2sup #287
pushed, but here we are.  Voted.

So, most pressing issue is about ending clean.  Which has a couple of
aspects:

 - Getting the 'quit' (status, packet, etc) delivered at the right
   time -- after the buffer drains appropriately.

 - Getting the 'half closed' status working systematically.

Ideally, I'd think we ought to be able to do a server side with like

    $ echo "813" | nc -NU /cdm/sockets/xfer.sock > rcvd.dat

where the echo closes after one line, so the server's endpoint should
be in a half-closed state before a client has even shown up?  That
seems wrong.  [75:

Sat Oct 17 00:30:06 2020 There's two connections involved on the
server side, right?  Well, the socket and the 'actual' connection
returned by the socket.  But the socket is supposed to stay open for
the life of the cdm.

So the 'server' side connection begins with the accept on the local
unix domain socket.  Once it's got a route it could go half-closed --
couldn't it? -- but stay available until it's processed a network
connection request, or the local connection goes away.

[76:

Sat Oct 17 02:49:41 2020 Jeez wait have to be more careful with the
two sides of the connection.  The

    $ echo "813" | nc -NU /cdm/sockets/xfer.sock > rcvd.dat

means that the local file handle will see eof after reading the
"813\n".  And we (where 'we' is netcat, here) can still write to
stdout after that, right?  Sure why not.

:76]

:75]

:74]
[77:

Sat Oct 17 07:19:57 2020 So I've been thinking more and more about how
one ought to represent state machines -- and in particular, how to
represent what specific actual state one is in.  Which I've
traditionally embodied as some sort of enum data member, but more and
more I'm thinking it ought to be a method returning an enum.

A _single_ _non-virtual_ method that examines the environment and the
working internals of the machine and decides what the current machine
state should be called.

With the key point, I guess, being that there is no explicit way to
write an arbitrary state transition.  You can't just say 'go to state
B'.  Instead, you have to make some alteration to the 'environment or
working internals' of the machine such that the 'getState()' function
will now return 'B'.

And so it might not be possible to go directly to state 'B' from the
current state:

 - Because it might require an environmental change you can't now do,

 - Because it might require changes that mean you are really going to
   state 'C', then 'D', and then finally 'B', and those intermediate
   states could be observable and/or have unexpected consequences.

[78:

Sat Oct 17 08:16:20 2020 So this is the idea of 'stigmergic state
machines'.

And I guess the structure of getState() is a pretty important part of
that -- it's the standing-start reexamination of the system state
that's (a big) key to robustness.  Because getState() is sequential,
it must look at _something_ first.  And whatever it looks at first
becomes critical in determining the state equivalence classes over the
entire phase space of the system.  Once we see something that says we
are in an 'early' state, like a reset state or whatever, we must not
be able to escape that without actually resetting any and all related
state as we move up.

[79:

Sat Oct 17 09:24:17 2020 So in the case at hand, how could this play
out.[80:

Sun Oct 18 01:34:28 2020 Gah so slow here.  Come ON.

getState for SRManager

 return NEED_CONFIGURE unless defined mSocketPath
 return NEED_OPEN unless defined mSocket and defined mSelect
 return NEED_CLEANUP if scalar mFinishedCxns
 return NEED_ACCEPT if mSelect..can_read..mSocket
 return NEED_CLOSE if env(shutting_down) ..yarite
 return NEED_NOTHING otherwise

getState for SREndpoint
 return NEED_CONNECT unless defined mSRManager, mUserHandle, mOurStartSeqno
 return

:80]

:79]

:78]

:77]
[81:

Sun Oct 18 07:09:06 2020

   userc          CDMC                                          CDMS        users
     .             run                                           run          .
     .              .                                            open         .
     .              .                                           listen        .
     .              .                                             .           .
     .              .                                             .      --connect
     .              .                                   SRE     accept<-/     .
     .              .      SRE                           .    ----.           .
  connect --\       .       .                          init<-/                .
     .       \-->accept     .                            .          /------ "82"
     .              .---    .                         configure<----          .
     .                  \>init                  mXmitData. (2,0)              .
     .                      .                            .                    .
   "578" -------\           .                            .                    .
     .           \----->configure                        .                    .
     .             mXmitData. (-2,0)                     .
     .                      .                            .
     .                    "578C"--\                      .
     .                      .      ------------------>acceptConnection
     .                      .                            .
     .                      .      /-------------------"318R"
     .          handleResponse<---/                      .
     .                      .mRecvData                   .











:81]
[82:

Sun Oct 18 08:29:06 2020 So how do we represent the half-closed
states?  And how does each arise, exactly?

(S1) local socket connect, creates SREndpoint
(S2) configure route, SREndpoint becomes known server

(S3) eof on local socket, server SREndpoint computes the final seqno
     that it wishes to xmit.

-> SREndpoint server is now half-closed.

(S4) It will continue to xmit any unacked data it has buffered, until 



:82]
[83:

Mon Oct 19 00:36:36 2020 Screw it let's move on the 'final seqno'
idea; that's got more stigmeric feel than the current mEOFSeen.

DO SOMETHING GO GO GO

:83]
[84:

Mon Oct 19 01:30:06 2020 OK so I want to push decision making closer
to the workers -- out of SREndpoint and into DataQueue.  We have all
these confident-sounding SREndpoint methods like isFullyClosed() which
turn out to be based on hack seqno manipulations.

SREndpoint::isFullyClosed() should do something like ask mXMitData if
it is closed for xmit and ask mRecvData if it closed for recv.

Now, I wanted to push the local file handle down, but DataQueue really
has no good place for it, and use cases where it's not even involved,
so that doesn't seem so good.

:84]
[85:

Mon Oct 19 01:44:19 2020 I think I'm going to have to
clean-slate-and-pull-pieces to get this sorted out.  Have to do a
spike just on opening and closing the ends in appropriate sequences,
then fit in the data carriage and stuff that's working into that.

But maye not this absolute instant, sigh.[86:

Tue Oct 20 01:55:58 2020 OK, how about this instant right now instead?

TODO

[87: Wed Oct 21 14:09:52 2020 In ./pud2.pl, vaguely
DONEish :87] - restart from pairunixdomain.pl
   = maybe don't use IO::Select, though?  Just do non-blocking read

 - spike entire lifecycle including half-closes

 - Make 'stigmergic sequencer' that proceeds through BORN,
   CONNECT, COMMUNICATE, CONCLUDE, DIE

 phase      behavior            state
 BORN:       initialize route   firstLine
   if local eof with empty first line    
     -> DIE()    
   if bad route at eof or eol
     -> DIE(err) 
   if complete good route
     -> CONNECT(route)
   if read newBytes  
     -> BORN(firstLine+newBytes) if localread > 0
   : GFN

 CONNECT     client<->server    route, isClient=, lastactivity=0, ipkt=undef
   if !isClient & ipkt==C etc
     -> COMMUNICATE(0,relc,route) 
   if isClient & ipkt==R etc
     -> COMMUNICATE(1,relc,route) 
   if age(lastact) > TIMEOUT
     send C
     -> CONNECT(route,isClient,now,ipkt)
   if local eof && isClient
     -> CONCLUDE() ?  DIE() ?
   : GFN
   

 COMMUNICATE  end<->end         xmitData/frontseq/lastseq, recvData/frontseq/lastseq
   if xd.frontseq == xd.lastseq && rd.frontseq == rd.lastseq
     -> CONCLUDE()
   if 
   { send/recv/retire data as implemented }

 DIE         end local cxn       msg
   if msg report to stderr
   : HEA

:86]

:85]
[88:

Wed Oct 21 14:10:23 2020 So it appears our understanding of 'nc -NU'
is right -- we can still print to the nc socket after reading eof on
that same socket, and the output appears on the nc side.

So.  Can we pull in DataQueue.pm here?  Or start a new minified DQ
from it?  How symmetric is it really?  How symmetric do we want to try
to force it to be?  If we go asymmetric, we could put both buffers
into a single BiDirNetPipe or something, and let the input and output
ends get customized for their directions.

Let's try a fresh start and try to stay squeaky; come back here when
things don't feel squeaky.[89:

Wed Oct 21 14:16:01 2020 If we're doing both directions in one, how is
this different than SREndpoint?
[90:

Wed Oct 21 14:39:54 2020 OK, we have a minimal EP.pm (for 'EndPoint'
arg bad camelcaps).  So far it's just holding the user handle and
handing it back, though.  What's next step?

 - EP::tryReadLocal

What needs to be true to do that?  Are we doing stigmergic actions
machines or anything like it, here?  Is tryReadLocal a low-level read
and return bytes thing?

I think tryReadLocal is the wrong thing to write now.  We want
lifecycle stuff first somehow.[91:

Wed Oct 21 14:48:21 2020 So, that's like update() or advance() or
something like that?  Maybe let's try.

:91]

:90]
:89]

:88]
[92:

Wed Oct 21 15:34:52 2020 So, how do we terminate an EP cleanly?
There's a foggen lifecycle issue for you.  The case at hand is local
EOF before getting a route.  At that point you might give a message,
but then you clean up and that EP is just done.

:92]
[93:

Wed Oct 21 17:53:10 2020 OK, making progress.  pud2.pl starts up,
accepts connections, recognizes when it's a server, configures route,
waits.  nc as client connects, waits, cleans up if nc quits.

[94:

Wed Oct 21 17:55:13 2020 Making multiple servers doesn't seem to work?
Hadn't tried that til just now..[95:

Thu Oct 22 00:46:32 2020 No, actually it did, it just wasn't printing
anything.

So how do we fake up more steps here?
[96:

Thu Oct 22 00:48:57 2020 In particular, all we've done so far is
interactions with the local unix socket under our own initiative.  And
that was already weird in that we had to be ready early to start
buffering local socket input, just so we could check for eof..

How do we deal with asynchronous packet input?  Same thing?  Buffer
the packets and just call that buffer state for the action machine to
deal with?

[97:

Thu Oct 22 01:35:28 2020 Stigmergy says we should be able to call our
action function 'at any time', so we could just say that when a packet
arrives that can be resolved onto a given EP, we should stick that
packet onto a one-packet data member part of the state, then call
update() right then and there.  And it's up to update to react to that
data member packet, or not, as appropriate.

We're not going to let update() be entered recursively, so we need to
be a little careful how thick we let its processing get.
[98:

Thu Oct 22 01:58:48 2020 OK so update blocks re-entry.  Starting to
write responding-to-inbound-packet code, but that's troublesome --
it's a ton more stuff to spike up from nothing, which makes me want to
start pulling from the existing modules.. but that feels
super-premature and risks squandering my 'spike clarity' before it's
really clear what I want.

Maybe I can start pulling in the PacketSR stuff without too-horrible
pollution..?[99:

Thu Oct 22 02:07:16 2020 Grr, we can maybe make packets, but we can't
send them without pulling in CDM.pm and that implies everything.

Can we gingerly carry EP.pm over to a 'spike-in-situ' in cdm?[100:

Thu Oct 22 10:34:01 2020 Well, urgh, started moving EP.pm into
apps/cdm/cdm, and things immediately start getting confusing and my
clean life-cycle stops being so clean.  GRRR!  On top of that, it's
getting to be time to move the flag

:100]

:99]

:98]
:97]

:96]
:95]

:94]

:93]
[101:

Fri Oct 23 02:03:12 2020 OK so let's push this through.  Integrate EP
into CDM.pm and then come say you did.  GO.[102:

Fri Oct 23 02:05:26 2020 So let's push the EPs into SRManager?  How
could that go?

 - Save a ref copy in SRManager.pm-hold

 - Still need two access paths to EPs: route-related (once a route
   exists) and fileno-related (lifetime of the local socket).

   = Say SRManager::mEPs for fileno -> EP.  This is the 'owning' link
   = Plus SRManager::mEPByKey for routekey -> This is a second ref

 - Let's try just replacing SREndpoint with EP, and see how far we can
   push it.

:102]

:101]
[103:

Fri Oct 23 02:52:06 2020 OK, we're starting to move forward with EP
instead of SREndpoint, and EPManager instead of SRManager.  Have
gotten as far as trying to send the PacketSR_C packet from the client
side.  

:103]
[104:

Fri Oct 23 12:11:57 2020 OK, well now we're getting to the nub of
things -- if we're going to do this 'input is just stigmergic state to
react to' thing, then it's time to get going.  We're trying to write
EP::maybeReceiveData.. and in the old SREndpoint version, the first
thing it does is check if it's an appropriate state to receive the
packet.  We could just write analogous state-checking code here
now.. or we could 'just' stash the packet in some new mInboundPkt data
member and then call EP::update().

If we dared.
[105:

Fri Oct 23 12:26:22 2020 I'M TOO CHICKEN!

:105]
:104]
[106:

Fri Oct 23 14:48:19 2020 OK, a fair amount of stuff is working, but
the ack/retirement stuff is not yet confirmed.  And the bigger
question is: Where do retransmits get handled in the current scheme?
Right now it seems we're only sending data packets when we have new
local stuff to ship, which means we don't send (empty) ACKs when our
side is quiet.

:106]
[107:

Fri Oct 23 15:01:05 2020 Breaking until the overnight..

:107]
[108:

Sat Oct 24 02:25:46 2020 OK, so, how do we know when to send an empty
data packet just to ack inbound stuff?

When they send us something it goes into our mFrontNetBuffer and
mFromNetAckedSeq increments.  

How do we know whether we've already sent a packet acking that changed
mFromNetAckedSeq?[109:

Sat Oct 24 02:32:33 2020 Ah hmm SREndpoint had mNeedToAck.  I guess EP
wants that too.[110:

Sat Oct 24 05:04:25 2020 Well, I guess adding a flag byte to
PacketSR_D.  That means we can ditch the Dd distinction and just have
an ISCLIENT flag.  

:110]

:109]

:108]
[111:

Sat Oct 24 09:04:59 2020 Well, getting closer.  Now we're closing one
side cleanly, but it's not shipping the LSQ flag notification before
it quits.  How do we want to resolve that?  It's really a
manifestation of the same infinite-regress-of-other-minds problem that
TCP has as well, and it hacks around with its CLOSE_WAIT, and just
foists the consequences out onto implementations and then the world.

[112:

Sat Oct 24 12:08:20 2020 Well so let's make a little bit of that and
move on, here.  state_NEED_WAIT?  What does it stig-merge on?
mLastActivity?  Maybe plus a repeat count?

[113:

Sat Oct 24 12:30:07 2020 So the specific issue here is that one side
or the other will be the first to not be in state_NEED_IO.  That side
will already have seen local EOF, so mToNetLastSeq will be set, and it
will receive a packet flagged with D_PKT_FLAG_LAST_SEQ, and at that
point mFromNetLastSeq will be set and it will no longer be in
state_NEED_IO. 
[114:

Sat Oct 24 12:38:33 2020 What if we set mNeedToAck when we set a
'lastseq' variable -- even either one, local or remote?  And say we
NEED_IO if mNeedToAck is true?[115:

Sat Oct 24 12:57:49 2020 OK, now we're sending a 'final' D pkt because
of mNeedToAck, but that D pkt contains the final seqno, which causes
us to set mNeedToAck again, and we never actually exit state_NEED_IO.

Can we set mNeedToAck only when we first acquire lastseq info?[116:

Sat Oct 24 13:10:34 2020 Woah, it kind of worked.  Looks like I just
had one extra mNeedToAck setting in there.  Both sides now exited
cleanly, once we saw EOFs from both local users.

Let's try another example.

..actually, let's try another go right in place, here.  With
clean-living connections we ought to be able to go right round again
without restarting cdm.pl..[117:

Sat Oct 24 13:13:14 2020 Buuuuut we can't.  Something failing to clean
up properly in EPManager.  '$self->{mKeyToEP}->{$key}' is still
defined.  How did that happen?[118:

Sat Oct 24 13:14:21 2020 Ahh, hmm.  What's wrong with this picture?

    delete $self->{mKeyToEP}->{$fileno};

I mean, why WOULDN'T someone delete a '$fileno' entry of a hash called
'mKeyToEP'? 

:118]

:117]



:116]

:115]

:114]
:113]

:112]

:111]
[119:

Sat Oct 24 13:22:26 2020 So okay!  Things finally starting to work
semi-plausibly here![120:

Sat Oct 24 13:32:22 2020 And we sent a file from the command line!
Behold:

SERVER SIDE
    root@beaglebone:~# echo 81 | nc -NU /cdm/sockets/xfer.sock > recv.dat
    root@beaglebone:~# 

CLIENT SIDE
    root@beaglebone:/home/t2/T2-12/notes# echo 58 | cat - /home/t2/T2-12/Makefile | nc -NU /cdm/sockets/xfer.sock 
    root@beaglebone:/home/t2/T2-12/notes# 

(and then, server side:

    root@beaglebone:~# ls -l recv.dat /home/t2/T2-12/Makefile
    -rw-r--r-- 1 t2   t2   2001 Sep 23 11:37 /home/t2/T2-12/Makefile
    -rw-r--r-- 1 root root 2001 Oct 24 13:31 recv.dat
    root@beaglebone:~# diff recv.dat /home/t2/T2-12/Makefile
    root@beaglebone:~# 
)

WAHOO!

[121:

Sat Oct 24 13:34:29 2020 I think we're not handling packet drops and
retries, though, in the current version.  So when we start suppressing
debug output and speeding up the timings, sooner or later we'll run
into trouble I think..

:121]

:120]

:119]
[122:

Sun Oct 25 04:04:37 2020 OK, we've upped the in-flight window to 1KB,
and sped up the timings slightly, and reduced the logging
signficantly.  Fixed a few bugs and things are still working-ish.  But
unlikely that we've actually experienced a packet drop, yet.  Let's
kick the window up to 4KB and see what happens..

:122]
[123:

Sun Oct 25 07:06:53 2020 OK so do we have really any clue how we want
to reset the far-end stream index after dropped packet(s)?  We now
have D_PKT_FLAG_RETRY_SEQ, but how do we want to use it?

Do we maybe want to distinguish between missed packets and redundant
packets?  We used to, but I folded that away just now.  Maybe unfold
again? [124:

Sun Oct 25 07:26:51 2020 OK now we distinguish DUPs from DROPPEDs, but
we still need better recovery.  I think we need a per-EP flag saying
when we're 'in sync', that's set whenever we get a packet we're
expecting, and is cleared whenever we detect a DROP and request a
retransmit.

Then the behavior is: If we detect a DROP when we're NOT in sync, we
assume we've already sent an D/RSQ, and we don't send another one.  If
that prior D/RSQ gets eaten we'll have to hit our backstop timeout
retry mechanism that I'm not 100% exists yet in this iteration..
[125:

Sun Oct 25 07:34:57 2020 So we currently have a behavior where we
might take only part of the data in a received D pkt, and that's
feeling like a mistake.

First off, we'd like our receive buffer limit to be well above the max
window in flight limit, so this really shouldn't happen.[126:

Sun Oct 25 08:50:03 2020 OK, we're really starting to stress things
here, running two (looped back) file transfers simultaneously.
Packets are getting dropped which is slowing the data rate, but it was
all managing to progress until I ^C'd one of the server 'nc' processes
and got

    382.65: Retiring 2160 to 224617 on dq route 18

    Debugger broken pipe

in the *gud-cdm.pl* debugger window.

[127:

Sun Oct 25 08:52:33 2020 So I think that means I need something like

    $SIG{PIPE} = sub { SOMETHING };

but not yet sure what SOMETHING we want.  Let's try just printing a
message first..[128:

Sun Oct 25 10:11:05 2020 So the current problem is that if our
inbound mFromNetBuffer is empty, we don't even try to syswrite to the
local process, and therefore the SIGPIPE that is waiting for us is not
delivered.

Could we try writing 0 bytes and see if it provokes the SIGPIPE?[129:

Sun Oct 25 10:20:31 2020 Well so far writing 0 bytes seems to be safe,
and it did provoke the SIGPIPE and allowed us to clean up.[130:

Sun Oct 25 10:29:49 2020 OK, so now we're cleaning up okay from
locally killing the client, but it leaves the server side hanging,
and then dying at

    if (defined($self->{mFromNetAckedSeq})) {
        return DPSTD($self->getDesc()." Ignoring duplicate connection $theirseq")
            if $theirseq == $self->{mFromNetAckedSeq};
        die "INCONSISTO ($theirseq vs $self->{mFromNetAckedSeq})";
    }

when another connection comes in.

So ideally we'd ship some kind of abort code before dying, if we
  could.[131:

Sun Oct 25 15:46:20 2020 Well, would we prefer something like
D_PKT_FLAG_ABORT, or something like PacketSR_Q?

D_PKT_FLAG_ABORT:
  Pro: We are already using D_PKT_FLAGs with some success
  Con: Might cause other effects on the way to recognizing it
  
PacketSR_Q:
  Pro: Offers a completely separate place to deal with this
  Con: Requires a whole new flow to deal with this
  
[132:

Sun Oct 25 17:49:28 2020 OK, time to break for dinner.  Current status
is PacketSR_Q is getting the server EP to shut down when we kill the
client side, but killing the server doesn't shut down the client EP.
[133:

Sun Oct 25 20:10:39 2020 Problem almost surely is our new PacketSR_Q
doesn't have a isClient/isServer flag, so we're looking up the wrong
endpoint key going in one direction or the other.  [134:

Sun Oct 25 20:19:03 2020 Yeah that was it; now PacketSR_Q has an
mFlags WTH, and ^C'ing is cleaning up the far end in both directions.
'Only' remaining issue is that the far end exits status 0 but it
should be something else.[136:

Mon Oct 26 01:23:40 2020 Um, yeah, except, the far end exit status is
not actually under our control, chumley.  The process that is exiting
is netcat, right?  All we can do is close the socket, and then nc does
with that whatever it's going to do.

All we could do would be to violate transparency and send some kind of
error message text via the socket, before closing it.

Screw that.  Far side user of nc won't know why it exited.  Users at
both ends will have to do file integrity some OoB way -- such as
layering some additional protocol on top of the nc-based data
carriage.

Like a first line containing a byte count and a newline, then that
many bytes, then a magic string and a file checksum.

Let's work on that instead.

:136]


But first sleep now I think.

[135:

Mon Oct 26 01:20:30 2020 OK let's 'polish' this off, get it committed,
and move on.
:135]
:134]

:133]
:132]


:131]

:130]

:129]

:128] 

:127]


:126]

:125]

:124]

:123]
[137:

Mon Oct 26 02:02:06 2020 Committing.

:137]
